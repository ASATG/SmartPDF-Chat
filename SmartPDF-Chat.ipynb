{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwN-XYJDWOVc"
      },
      "outputs": [],
      "source": [
        "!pip install langchain huggingface_hub tiktoken\n",
        "!pip install chromadb\n",
        "!pip install PyPDF2 InstructorEmbedding\n",
        "!pip install --upgrade together"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "UOCZLH4xWtdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "r33h9GDsXsWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "# set your API key\n",
        "together.api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
        "\n",
        "# list available models and descriptons\n",
        "models = together.Models.list()"
      ],
      "metadata": {
        "id": "yIReW1VHXuPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "import logging\n",
        "from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "from pydantic import Extra, Field, root_validator\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "from langchain.utils import get_from_dict_or_env\n",
        "\n",
        "class TogetherLLM(LLM):\n",
        "    \"\"\"Together large language models.\"\"\"\n",
        "\n",
        "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
        "    \"\"\"model endpoint to use\"\"\"\n",
        "\n",
        "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
        "    \"\"\"Together API key\"\"\"\n",
        "\n",
        "    temperature: float = 0.1\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    max_tokens: int = 1024\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    class Config:\n",
        "        extra = Extra.forbid\n",
        "\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the API key is set.\"\"\"\n",
        "        api_key = get_from_dict_or_env(\n",
        "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
        "        )\n",
        "        values[\"together_api_key\"] = api_key\n",
        "        return values\n",
        "\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"together\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call to Together endpoint.\"\"\"\n",
        "        together.api_key = self.together_api_key\n",
        "        output = together.Complete.create(prompt,\n",
        "                                          model=self.model,\n",
        "                                          max_tokens=self.max_tokens,\n",
        "                                          temperature=self.temperature,\n",
        "                                          )\n",
        "        text = output['output']['choices'][0]['text']\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "K7y4v-EIX0z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers==2.2.2"
      ],
      "metadata": {
        "id": "gHQRARxvY6wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings"
      ],
      "metadata": {
        "id": "jACJM9NlX6bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
        "                                                      model_kwargs={\"device\": \"cuda\"})\n"
      ],
      "metadata": {
        "id": "IAzn4TSnX94s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "ynZ2eiR9X5A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "co_hamachar_zaky_file_id=\"1d2Y_rjcU8wqQ8sMsk7i1cizcfnUF0rdb\"\n",
        "url = f\"https://drive.google.com/uc?id={co_hamachar_zaky_file_id}\"\n",
        "output_file = \"co_hamachar_zaky.pdf\"\n",
        "gdown.download(url, output_file, quiet=False)\n",
        "\n",
        "co_william_stallings_file_id=\"1UEHIoEMBg_1o8b5ckFvTdAZmrTyAVqAj\"\n",
        "url = f\"https://drive.google.com/uc?id={co_william_stallings_file_id}\"\n",
        "output_file = \"co_william_stallings.pdf\"\n",
        "gdown.download(url, output_file, quiet=False)"
      ],
      "metadata": {
        "id": "WbXGE416aFL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "co_hamachar_zaky_reader=PdfReader('/content/co_hamachar_zaky.pdf')\n",
        "co_william_stallings_reader=PdfReader('/content/co_william_stallings.pdf')"
      ],
      "metadata": {
        "id": "giXUqnbLa2LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordninja"
      ],
      "metadata": {
        "id": "BDRdWyiSmLJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wordninja\n",
        "\n",
        "start_and_end={\"co_hamachar_zaky\":[292,347,co_hamachar_zaky_reader],\"co_william_stallings\":[146,249,co_william_stallings_reader]}\n",
        "\n",
        "mapping={}\n",
        "all_chunks=[]\n",
        "\n",
        "for key in start_and_end:\n",
        "    start_page=start_and_end[key][0]\n",
        "    end_page=start_and_end[key][1]\n",
        "\n",
        "    for page_number in range(start_page - 1, end_page, 1):\n",
        "        page = start_and_end[key][2].pages[page_number]\n",
        "        temp_text=\" \".join(wordninja.split(page.extract_text()))\n",
        "        temp_chunks=RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1024, chunk_overlap=250, length_function=len\n",
        "            ).split_text(temp_text)\n",
        "        mapping[key+\"_\"+str(page_number+1)]=temp_chunks\n",
        "        all_chunks.extend(temp_chunks)"
      ],
      "metadata": {
        "id": "hnibm4NJbeSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_chunks)"
      ],
      "metadata": {
        "id": "lu1qQAlKdNiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_texts(texts=all_chunks,embedding=instructor_embeddings)"
      ],
      "metadata": {
        "id": "q9skO3j5dQB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "oGB1hChzdl-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-70b-chat\",\n",
        "    temperature = 0.8,\n",
        "    max_tokens = 2000\n",
        ")"
      ],
      "metadata": {
        "id": "4ymXP-S_YiR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "zPeCmtNeYlDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cite sources\n",
        "\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))"
      ],
      "metadata": {
        "id": "16w1kLiMYrss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# full example\n",
        "\n",
        "base_query=\"Explain me memory architecure\"\n",
        "\n",
        "ans=vectordb.similarity_search_with_relevance_scores(query=base_query, k=10,score_threshold=0.6)\n",
        "\n",
        "for item in ans:\n",
        "    for page in mapping:\n",
        "        if str(item[0])[14:-1] in mapping[page]:\n",
        "            print(item[1],page,'->',str(item[0])[14:-1],end='\\n\\n')"
      ],
      "metadata": {
        "id": "rY3sNCsOYuFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response = qa_chain(base_query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "zeBpuwYunqas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhREccXPp7eU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}